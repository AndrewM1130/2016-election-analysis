---
title: "U.S. Presidential Election Polling"
author: 'Andrew Ma'
output:
  html_document:
    df_print: paged
    keep_md: yes
---

<!-- # Instructions and Expectations -->

<!-- - You are allowed and encouraged to work with two partners on this project.  Include your names, perm numbers, and whether you are taking the class for 131 or 231 credit. -->

<!-- - You are welcome to write up a project report in a research paper format -- abstract, introduction, methods, results, discussion -- as long as you address each of the prompts below.  Alternatively, you can use the assignment handout as a template and address each prompt in sequence, much as you would for a homework assignment. -->

<!-- - There should be no raw R _output_ in the body of your report!  All of your results should be formatted in a professional and visually appealing manner. That means that visualizations should be polished -- aesthetically clean, labeled clearly, and sized appropriately within the document you submit, tables should be nicely formatted (see `pander`, `xtable`, and `kable` packages). If you feel you must include raw R output, this should be included in an appendix, not the main body of the document you submit.   -->

<!-- - There should be no R _codes_ in the body of your report! Use the global chunk option `echo=FALSE` to exclude code from appearing in your document. If you feel it is important to include your codes, they can be put in an appendix. -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, 
                      cache = T,
                      fig.align = 'center',
                      fig.height = 4, 
                      fig.width = 4,
                      results = 'markup')

library(pander)
library(tidyverse)
library(ggmap)
library(modelr)
library(maps)
library(ggplot2)
library(leaflet)
library(caret)
library(ROCR)
library(tree)
library(maptree)
library(class)
library(pROC)
```

# Background

The U.S. presidential election in 2012 did not come as a surprise. Some correctly predicted the outcome of the election correctly including [Nate Silver](https://en.wikipedia.org/wiki/Nate_Silver), 
and [many speculated about his approach](https://www.theguardian.com/science/grrlscientist/2012/nov/08/nate-sliver-predict-us-election).

Despite the success in 2012, the 2016 presidential election came as a 
[big surprise](https://fivethirtyeight.com/features/the-polls-missed-trump-we-asked-pollsters-why/) 
to many, and it underscored that predicting voter behavior is complicated for many reasons despite the tremendous effort in collecting, analyzing, and understanding many available datasets.

<!-- Your final project will be to merge census data with 2016 voting data to analyze the election outcome.  -->

<!-- To familiarize yourself with the general problem of predicting election outcomes, read the articles linked above and answer the following questions. Limit your responses to one paragraph for each. -->

1. What makes voter behavior prediction (and thus election forecasting) a hard problem?

$\rightarrow$ **Voter behavior prediction is a hard problem to model and predict because it involves human emotions including shame or guilt. Consequently, potential voters may lie or misrepresent their true vote when presented with the traditional polling methods. For context, this misclassification/ wrong prediction is not unexpected as a majority of U.S. presidential models have had poor accuracy in recent years; to acquire more accurate voter predictions and election forecasting results, one would have to improve on the traditional media polling methods involving modern survey methods through smartphones or websites.  **

2. What was unique to Nate Silver's approach in 2012 that allowed him to achieve good predictions?

$\rightarrow$ **Nate Silver's approach in 2012 had an accuracy rate of 100% - accurate predictions on voter behavior in every single state. To achieve such results, Silver employed a model involving Bayesion priors and selecting from a range of percentages; another way to describe it would be almost like a decision tree approach where you input previous known alongisde unknowns to arrive at percentages with different probabilities. I think that this spotlight on Silver's statistical model glosses over its inherent simplicity, and thus, intrepretebility; in such a technologically advanced world with these fancy models and computational methods, simplicity is sometimes refreshing.  **

3. What went wrong in 2016? What do you think should be done to make future predictions better?

$\rightarrow$ **In summary, there are numerous unique reasons which result in the error with classification models; in 2016 specifically, I would hypothesize that tremendous social pressure and social culture resulted in an over-fitted model due and a bias towards Clinton as the voter feelings were improperly assumed. To make future predictions better, I would advise pollers to consider the implementation of online anonymous polls or to look outside of the box and consider different people or to incorporate these variances into their statistical model rather than just basing their models off basic polling. **

# Data

The `project_data.RData` binary file contains three datasets: tract-level 2010 census data, stored as `census`; metadata `census_meta` with variable descriptions and types; and county-level vote tallies from the 2016 election, stored as `election_raw`.
```{r}
load('project_data.RData')
```

## Election data

Some example rows of the election data are shown below:
```{r}
filter(election_raw, !is.na(county)) %>% 
  head() %>% 
  pander()
```

The meaning of each column in `election_raw` is self-evident except `fips`. The accronym is short for [Federal Information Processing Standard](https://en.wikipedia.org/wiki/FIPS_county_code). In this dataset, `fips` values denote the area (nationwide, statewide, or countywide) that each row of data represent.

Nationwide and statewide tallies are included as rows in `election_raw` with `county` values of `NA`. There are two kinds of these summary rows:

* Federal-level summary rows have a `fips` value of `US`.
* State-level summary rows have the state name as the `fips` value.

4. Inspect rows with `fips=2000`. Provide a reason for excluding them. Drop these observations -- please write over `election_raw` -- and report the data dimensions after removal. 

$\bullet$ **We drop rows with "fips == 2000" because a fips value of 2000 has no corresponding county data - it is a null row. The dimensions of election_raw are 18345 rows by 5 columns after dropping these observations.**

```{r}
election_raw <- election_raw %>% subset(fips != 2000)

election_raw %>% dim()
```


## Census data

The first few rows and columns of the `census` data are shown below.
```{r}
census %>% 
  select(1:6) %>% 
  head() %>% 
  pander(digits = 15)
```
Variable descriptions are given in the `metadata` file. The variables shown above are:

```{r}
census_meta %>% head() %>% pander()
```

\newpage
## Data preprocessing

5. Separate the rows of `election_raw` into separate federal-, state-, and county-level data frames:

    * Store federal-level tallies as `election_federal`.
```{r}
election_federal <- election_raw %>%
  subset(fips == 'US')
```
    
    
    * Store state-level tallies as `election_state`.
```{r}
election_state <- election_raw %>% 
  filter(nchar(fips) == 2, fips != "US")
```
    
    
    * Store county-level tallies as `election`. Coerce the `fips` variable to numeric.
```{r}
election <- election_raw %>%
  filter(nchar(fips) != 2)

election$fips <- election$fips %>% as.numeric()
```
    

6. How many named presidential candidates were there in the 2016 election? Draw a bar graph of all votes received by each candidate, and order the candidate names by decreasing vote counts. (You may need to log-transform the vote axis.)
```{r fig.width = 8}
#get number of unique candidates
election_raw$candidate %>% 
  unique() %>%
  length()

#bar graph of votes vs. candidate
election_federal %>% ggplot(aes(x = log(votes), y = reorder(candidate, votes))) + 
  ggtitle("2016 Presidential Elections") +
  ylab("Presidential Candidates") +
  xlab("Log-scaled Votes(Millions)") +
  geom_bar(stat="identity") +
  theme_bw()
```

$\rightarrow$ **Using the count() and unique() functions, we know there are at least 31 named presidential candidates, and one column containing "None of these candidates" in the 2016 election. Each of these candidates are displayed alongside their log - transformed respective vote count in the bar graph above. **


7. Create variables `county_winner` and `state_winner` by taking the candidate with the highest proportion of votes. (Hint: to create `county_winner`, start with `election`, group by `fips`, compute `total` votes, and `pct = votes/total`. Then choose the highest row using `slice_max` (variable `state_winner` is similar).)
```{r}
county_winner <- election %>%
  group_by(fips) %>%
  mutate(total = sum(votes),
         pct = votes/total) %>%
  slice_max(pct, n = 1)


state_winner <- election_state %>%
  filter(fips == state) %>%
  group_by(fips) %>%
  mutate(total = sum(votes), 
         pct = votes/total) %>%
  slice_max(pct, n = 1)
```


# Visualization

Here you'll generate maps of the election data using `ggmap`. The .Rmd file for this document contains codes to generate the following map.
```{r}
states <- map_data("state")
par(mfrow = c(2,1))

ggplot(states) + 
  geom_polygon(aes(x = long, 
                   y = lat, 
                   fill = region, 
                   group = group), 
               color = "white") + 
  coord_fixed(1.3) + # avoid stretching
  guides(fill='none') + # no fill legend
  theme_nothing() # no axes

county <- map_data("county")

ggplot(county) + 
  geom_polygon(aes(x = long, 
                   y = lat, 
                   fill = region, 
                   group = group), 
               color = "white") + 
  coord_fixed(1.3) + # avoid stretching
  guides(fill='none') + # no fill legend
  theme_nothing() # no axes
```

8. Draw a county-level map with `map_data("county")` and color by county.
```{r}
county <- map_data("county")

ggplot(county) + 
  geom_polygon(aes(x = long, 
                   y = lat, 
                   fill = region, 
                   group = group), 
               color = "white") + 
  coord_fixed(1.3) + # avoid stretching
  guides(fill='none') + # no fill legend
  theme_nothing() # no axes
```


In order to map the winning candidate for each state, the map data (`states`) must be merged with with the election data (`state_winner`).

The function `left_join()` will do the trick, but needs to join the data frames on a variable with values that match. In this case, that variable is the state name, but abbreviations are used in one data frame and the full name is used in the other.

9. Use the following function to create a `fips` variable in the `states` data frame with values that match the `fips` variable in `election_federal`.
```{r, echo = T}
name2abb <- function(statename){
  ix <- match(statename, tolower(state.name))
  out <- state.abb[ix]
  return(out)}

#creating 'fips' variable in 'states' data-frame with requirements above & mutating it in 
states <- states %>% mutate(fips = name2abb(states$region))

#merging states and state_winner via left_join()
states_new <- left_join(states, state_winner)
```

Now the data frames can be merged. `left_join(df1, df2)` takes all the rows from `df1` and looks for matches in `df2`. For each match, `left_join()` appends the data from the second table to the matching row in the first; if no matching value is found, it adds missing values.

10. Use `left_join` to merge the tables and use the result to create a map of the election results by state. Your figure will look similar to this state level [New York Times map](https://www.nytimes.com/elections/results/president). (Hint: use `scale_fill_brewer(palette="Set1")` for a red-and-blue map.)
```{r}
#election results, filled by winner of each state
ggplot(states_new) + 
  geom_polygon(aes(x = long, 
                   y = lat, 
                   fill = candidate, 
                   group = group), 
               color = "white") + 
  coord_fixed(1.3) + 
  guides(fill= 'none') + 
  scale_fill_brewer(palette="Set1") +  
  theme_nothing() 
```


11. Now create a county-level map. The county-level map data does not have a `fips` value, so to create one, use information from `maps::county.fips`: split the `polyname` column to `region` and `subregion` using `tidyr::separate`, and use `left_join()` to combine `county.fips` with the county-level map data. Then construct the map. Your figure will look similar to county-level [New York Times map](https://www.nytimes.com/elections/results/president).

[Github](https://gitHub.com/AndrewM1130/2016-Election-Polling)

```{r}
#creating fips value column for county dataset
county <- map_data("county")

county_fips <- maps::county.fips %>%
  separate(polyname, c("region", "subregion"), sep = ",")

#adding county fips columns into county df 
county <- left_join(county, county_fips, by = "subregion")

#merging dataframes for visualization 
county_new <- left_join(county, county_winner, by = "fips")

#ggplot visualization code (color map by county)
ggplot(county_new) + 
  geom_polygon(aes(x = long, 
                   y = lat, 
                   fill = candidate, 
                   group = group), 
               color = "white") + 
  coord_fixed(1.3) +
  guides(fill= 'none') +
  scale_fill_brewer(palette="Set1") + 
  theme_nothing()
```

  
12. Create a visualization of your choice using `census` data. Many exit polls noted that [demographics played a big role in the election](https://fivethirtyeight.com/features/demographics-not-hacking-explain-the-election-results/). If you need a starting point, use [this Washington Post article](https://www.washingtonpost.com/graphics/politics/2016-election/exit-polls/) and [this R graph gallery](https://www.r-graph-gallery.com/) for ideas and inspiration.

$\bullet$ **With respect to the census data, I have decided to create a visualization exploring the relationship between poverty and political party affiliation. Notice from the graph that counties and states who voted Republican in 2016 suffers from higher levels of poverty when compared to counties who had voted majority Democratic. **

```{r fig.width=8}
#merging census data with long & latitude 
census_new <- census  %>% 
  select(State, County, Poverty) %>% 
  mutate(State = tolower(State),
         County = tolower(County))

colnames(census_new) <- c("region", "subregion", "poverty")

#new df with map data merged on
county_new <- county_new %>% 
  select(long,lat,subregion)
```


```{r fig.width=8}
#customized ggplot of census data
census_new %>% 
  group_by(region) %>%
  summarize_if(is.numeric, sum, na.rm=TRUE) %>%
  ggplot(aes(x = log(poverty), y = reorder(region, poverty)))+
  geom_bar(stat="identity") +
  ggtitle("Poverty Rates by State") +
  ylab("States") +
  xlab("Log-scaled Poverty Rate(%)") 
```

13. The `census` data contains high resolution information (more fine-grained than county-level). Aggregate the information into county-level data by computing population-weighted averages of each attribute for each county by carrying out the following steps:
    
* Clean census data, saving the result as `census_del`: 
  
   + filter out any rows of `census` with missing values;
   + convert `Men`, `Employed`, and `Citizen` to percentages;
   + compute a `Minority` variable by combining `Hispanic`, `Black`, `Native`, `Asian`, `Pacific`, and remove these variables after creating `Minority`; and
   + remove `Walk`, `PublicWork`, and `Construction`.
```{r}
census_del <- census %>%
  filter(complete.cases(.)) %>%
  mutate(Men = 100* (Men/TotalPop),
         Women = 100 * (Women/TotalPop),
        Employed = 100 * (Employed/TotalPop),
        Citizen = 100 * (Citizen/TotalPop),
        Minority = Hispanic + Black + Native + Asian + Pacific) %>%
  select(-c("Hispanic", "Black", "Native", "Asian", "Pacific", "Walk", "PublicWork","Construction" ))
```

 
* Create population weights for sub-county census data, saving the result as `census_subct`: 
    + group `census_del` by `State` and `County`;
    + use `add_tally()` to compute `CountyPop`; 
    + compute the population weight as `TotalPop/CountyTotal`;
    + adjust all quantitative variables by multiplying by the population weights.
```{r}
census_subct <- census_del %>%
  group_by(State, County) %>%
  add_tally(TotalPop) %>%
  mutate(CountyPop = n) %>%
  mutate(PopWeight = TotalPop/CountyPop) %>%
  select(-n, -TotalPop)

#adjusting all qualitative variables by population weight
census_subct[4:31] <- census_subct[4:31] * t(census_subct$PopWeight) 
```
  
    
* Aggregate census data to county level, `census_ct`: group the sub-county data `census_subct` by state and county and compute population-weighted averages of each variable by taking the sum (since the variables were already transformed by the population weights)
```{r}
census_ct <- census_subct %>%
  group_by(State, County) %>%
  summarize_if(is.numeric, sum, na.rm=TRUE)
```

    
* Print the first few rows and columns of `census_ct`. 
```{r}
census_ct %>% head()
```


14. If you were physically located in the United States on election day for the 2016 presidential election, what state and county were you in? Compare and contrast the results and demographic information for this county with the state it is located in. If you were not in the United States on election day, select any county. Do you find anything unusual or surprising? If so, explain; if not, explain why not.
```{r}
census_ct %>% 
  select(-CensusTract) %>%
  filter(State == "California", County == "Alameda")
```


$\rightarrow$ **For the 2016 presidential election, I was living in the Alameda County of Berkeley, California - results for Alameda county were: 14.54% Republican & 78.06% Democratic. With respect to the demographics of Alameda county, our census_ct dataframe tell us that Alameda has an even split between genders, there are a number of unique circumstances. For example it is clear that Alameda County is largely inhabited by minorities as they are 62.5% of the total county population. Furthermore, ~35.5% of inhabitants are actually non-citizens, but poverty rates and income levels are not bad at 10% and ~80k/yr respectively. **

# Exploratory analysis

15. Carry out PCA for both county & sub-county level census data. Compute the first two principal components PC1 and PC2 for both county and sub-county respectively. Discuss whether you chose to center and scale the features and the reasons for your choice. Examine and interpret the loadings.

$\bullet$ **PCA involves the reduction of dimensions and the size of our dataset while attempting to retain most of our information; digging deeper into the fundamentals behind PCA reveals that centering is done inherently through the SVD() function due to its relationship with variance, and scaling is similar to the normalization of our data. Through the exploratory analysis of the census data above, I would hypothesize that features will need to be both scaled and centered due to the large range of numeric values present within the dataset. Without scaling, we might lose a lot of intrepretibility in the graphs, which is crucial in a project surrounding presidential elections. **

```{r}
# PCA analysis for sub-county data
x_mx <- census_subct %>% 
  ungroup(State, County) %>% 
  select(-c("CensusTract","State","County")) %>%
  scale(scale = T)

# compute SVD
x_svd <- svd(x_mx)

# get loadings
v_svd <- x_svd$v

#compute firs two PC's for sub-county data
z_mx <- x_mx %*% x_svd$v

#plot of first two PC's
v_svd[, 1:2] %>%
  as.data.frame() %>%
  rename(PC1 = V1, PC2 = V2) %>%
  mutate(variable = colnames(x_mx)) %>%
  gather(key = 'PC', value = 'Loading', 1:2) %>%
  arrange(variable) %>%
  ggplot(aes(x = variable, y = Loading)) +
  geom_point(aes(shape = PC)) +
  theme_bw() +
  geom_hline(yintercept = 0, color = 'blue') +
  geom_path(aes(linetype = PC, group = PC)) +
  theme(axis.text.x = element_text(angle = 90)) +
  ggtitle("Loadings for Sub-County") + 
  theme(plot.title = element_text(hjust = 0.5)) +
  labs(x = '')
```

$\rightarrow$ **From PC1 and PC2, we can tell that ChildPoverty, Minority, and Unemployment features have the largest impact on the variance of our data; conversely, FamilyWork, SelfEmploted, and WorkAtHome had the least contribution. **

```{r }
# PCA analysis for county data
y_mx <- census_ct %>%
  ungroup(State, County) %>% 
  select(-c("State","County")) %>%
  scale(scale = T)

# compute SVD
y_svd <- svd(y_mx)

# get loadings
v_svd2 <- y_svd$v

#compute first two PC's for county data
z_mx1 <- y_mx %*% y_svd$v

#plot of first two PC's
v_svd2[, 1:2] %>%
  as.data.frame() %>%
  rename(PC1 = V1, PC2 = V2) %>%
  mutate(variable = colnames(y_mx)) %>%
  gather(key = 'PC', value = 'Loading', 1:2) %>%
  arrange(variable) %>%
  ggplot(aes(x = variable, y = Loading)) +
  geom_point(aes(shape = PC)) +
  theme_bw() +
  geom_hline(yintercept = 0, color = 'blue') +
  geom_path(aes(linetype = PC, group = PC)) +
  theme(axis.text.x = element_text(angle = 90)) +
  ggtitle("County Loadings") +
  theme(plot.title = element_text(hjust = 0.5)) + 
  labs(x = '')
```

$\rightarrow$ **With respect to the first two principal components of the county data, it seems that Employment, Income, and Professional are the features with the largest absolute value, while Poverty and Unemployment played smaller roles in affecting total variance. **

16. Determine the minimum number of PCs needed to capture 90% of the variance for both the county and sub-county analyses. Plot the proportion of variance explained and cumulative variance explained for both county and sub-county analyses.
```{r fig.height=4}
pc_vars <- x_svd$d^2/(nrow(x_mx) - 1)

# scree and cumulative variance plots for sub-county data
tibble(PC = 1:min(dim(x_mx)),
       Proportion = pc_vars/sum(pc_vars),
       Cumulative = cumsum(Proportion)) %>%
  gather(key = 'measure', value = 'Variance Explained', 2:3) %>%
  ggplot(aes(x = PC, y = `Variance Explained`)) +
  geom_point() +
  geom_path() +
  facet_wrap(~ measure) +
  ggtitle("Sub-County Loadings") +
  theme(plot.title = element_text(hjust = 0.5)) + 
  theme_bw() + 
  scale_x_continuous()
```

$\rightarrow$ **With respect to subcounty data, we would need ~15 principal components to capture 90% of the variance. **

```{r}
pc_vars <- y_svd$d^2/(nrow(y_mx) - 1)

# scree and cumulative variance plots for county data
tibble(PC = 1:min(dim(y_mx)),
       Proportion = pc_vars/sum(pc_vars),
       Cumulative = cumsum(Proportion)) %>%
  gather(key = 'measure', value = 'Variance Explained', 2:3) %>%
  ggplot(aes(x = PC, y = `Variance Explained`)) +
  geom_point() +
  geom_path() +
  facet_wrap(~ measure) +
  theme_bw() +
  scale_x_continuous()
```

$\rightarrow$ **From the Variance vs. PC graph of County, ~17 principal components are needed to capture 90% of the variance. ** 


17. With `census_ct`, perform hierarchical clustering with complete linkage.  Cut the tree to partition the observations into 10 clusters. Re-run the hierarchical clustering algorithm using the first 5 principal components the county-level data as inputs instead of the original features. Compare and contrast the results. For both approaches investigate the cluster that contains San Mateo County. Which approach seemed to put San Mateo County in a more appropriate cluster? Comment on what you observe and discuss possible explanations for these observations.
```{r}
# compute distances between points
d_mx <- dist(y_mx, method = 'euclidean')

# compute hierarchical clustering
hclust_out <- hclust(d_mx, method = 'complete')

# cut at 10 clusters
clusters <- cutree(hclust_out, k = 10) %>% 
  factor(labels = paste('cluster', 1:10))

# count number of data points per cluster
clusters %>% table()
```
```{r}
distance.5 <- dist(scale(data.frame(z_mx1[,1:5])))

hc.ct <- hclust(distance.5, method="complete")

cluster.5 <- cutree(hc.ct , k = 10) %>% 
  factor(labels = paste('cluster', 1:10))

cluster.5 %>% table()
```

```{r}
#finding cluster containing San Mateo
clusters[which(census_ct$County == "San Mateo")]
```

```{r}
#finding cluster containing San Mateo (2nd cluster method)
cluster.5[which(census_ct$County == "San Mateo")]
```

```{r}
hclust_avg <- hclust_out
plot(hclust_avg)
cut_avg <- cutree(hclust_avg, k = 2)
rect.hclust(hclust_avg , k = 2, border = 2:6)
abline(h = 2, col = 'red')
```

```{r}
#plot(hc.ct)
hclust_avg <- hc.ct
plot(hclust_avg)
cut_avg <- cutree(hclust_avg, k = 2)
rect.hclust(hclust_avg , k = 2, border = 2:6)
abline(h = 2, col = 'red')
```



$\rightarrow$ **Above, we first applied the hierarchical clustering method 10 clusters on our census_ct observations, then again with the first five principal components as the data. After splitting into clusters, we examined cluster sizes and located the cluster containing the "San Mateo" county observation. Notice that both methods result in drastically different cluster sizes as well as different locations for our 'San Mateo' observation. Closer examination reveals that clisters built withcensus_ct contain many Caliofornia counties and seem to be grouped by location, while the clusters created off the first five principal components have no obvious conclusions. Using census_ct, with San Mateo County in cluster 2, is a more appropriate cluster than using the first five principal components, with San Mateo in cluster 7, because of the relative similarities it has to California county numbers in cluster 2. Possible explanations of this difference may include the fact that 5 principal components does not capture enough variance of the data and does not provide an accurate model.**

# Classification

In order to train classification models, we need to combine `county_winner` and `census_ct` data. This seemingly straightforward task is harder than it sounds. Codes are provided in the .Rmd file that make the necessary changes to merge them into `election_county` for classification.
```{r}
abb2name <- function(stateabb){
  ix <- match(stateabb, state.abb)
  out <- tolower(state.name[ix])
  return(out)
}

tmpwinner <- county_winner %>%
  ungroup %>%
  # coerce names to abbreviations
  mutate(state = abb2name(state)) %>%
  # everything lower case
  mutate(across(c(state, county), tolower)) %>%
  # remove county suffixes
  mutate(county = gsub(" county| columbia| city| parish", 
                       "", 
                       county)) 

tmpcensus <- census_ct %>% 
  select(-"CensusTract") %>%
  mutate(State = tolower(State), County = tolower(County))

election_county <- tmpwinner %>%
  left_join(tmpcensus, 
            by = c("state"="State", "county"="County")) %>% 
  na.omit()

## save meta information
election_meta <- election_county %>% 
  select(c(county, fips, state, votes, pct, total))

## save predictors and class labels
election_county <- election_county %>% 
  select(-c(county, fips, state, votes, pct, total))
```

After merging the data, partition the result into 80% training and 20% testing partitions.
```{r}
#splitting election_county df into 80/20 training & test data sets
set.seed(1)
dt <- sort(sample(nrow(election_county), nrow(election_county) * 0.7))
train <- election_county[dt,] %>% as_tibble()
test <- election_county[-dt,] %>% as_tibble()
```


18. Decision tree: train a decision tree on the training partition, and apply cost-complexity pruning. Visualize the tree before and after pruning. Estimate the misclassification errors on the test partition, and interpret and discuss the results of the decision tree analysis. Use your plot to tell a story about voting behavior in the US (see this [NYT infographic](https://archive.nytimes.com/www.nytimes.com/imagepages/2008/04/16/us/20080416_OBAMA_GRAPHIC.html)).
```{r}
nmin <-50

nfold <-10

folds <- sample(cut(1:nrow(train), breaks=nfold, labels=FALSE))

tree_opts <- tree.control(nobs = nrow(train), 
                          minsize = nmin, 
                          mindev = exp(-6))

train$candidate <- train$candidate %>% as.factor()

#un-pruned tree
election.tree <- tree(candidate ~ . , data = train, split = 'deviance')

# cost-complexity pruning
cvtree <- cv.tree(election.tree)

# choose optimal alpha
best_alpha <- min(cvtree$size[which(cvtree$dev==min(cvtree$dev))])

# select final tree
prunedtree <- prune.tree(election.tree, k = 13, method="misclass") 
```

$\bullet$ **After closer analysis, we have determined that an alpha value of 13 will lead to the least amount of impurity within our pruned tree.**

```{r fig.width = 12, fig.height = 14}
par(mfrow=c(2, 1))

#plotting both trees
draw.tree(election.tree)
title("Unpruned Tree")

draw.tree(prunedtree)
title("Pruned Tree")
```


 $\rightarrow$ **It appears that the variables used to determine the tree are Transit, White, Unemployment, County Total,, Employed. White reappears within the tree indicating that it is an important factor that results in favorable results for Trump. Employment/Unemployment is another large factor that appears to trend towards more employed areas vote for Clinton over Trump.**
 
19. Train a logistic regression model on the training partition to predict the winning candidate in each county and estimate errors on the test partition. What are the significant variables? Are these consistent with what you observed in the decision tree analysis? Interpret the meaning of one or two significant coefficients of your choice in terms of a unit change in the variables. Did the results in your particular county (from question 14) match the predicted results?  
```{r warning = FALSE}
train$candidate <- train$candidate %>% as.factor()

                                                                     
fit_glm <- glm(candidate ~ . , data = train, family = 'binomial')

y <- train %>% pull(candidate)

# compute estimated probabilities
p_hat_glm <- predict(fit_glm, train, type = 'response')

# Bayes classifier
y_hat_glm <- factor(p_hat_glm > 0.5, labels = c("Donald Trump", "Hillary Clinton"))

# errors
error_glm <- table(y = y, y_hat_glm) %>% show()
```

$\rightarrow$ **From the misclassification table above, we conclude that the accuracy of our logistic regression model is pretty high with an accuracy rate of 0.9297348 on the test dataset, and with a relatively low false positive rate of 0.175. I believe that this high error rate arises from a lack of proper splitting techniques - maybe we should consider splitting via the caret() package or with replacement? When compared with Question 14, our model correctly predicted a "Clinton" classification for the candidacy response, which aligns with the actual 2016 electoral results in Alameda County. **

```{r}
fit_glm %>% summary()
```

$\rightarrow$ **Furthermore, we would like to dive deeper into the predictor variables and their impact on our logistic regression model are those with low p-values such as Citizenship, Unemployment,  ; surprisingly, variables such as Driving, Carpooling, and Population density also have large effects on our response. Conversely, predictors such as transit type and self-employment had the lowest amount of impact and correlation on our response. Finally, one interpretation that can be made from the summary of our logistic regression is: For every one unit increase of poverty, the log odds of Donald Trump being the favored candidate increases by 0.011. **

20.  Compute ROC curves for the decision tree and logistic regression using predictions on the test data, and display them on the same plot. Based on your classification results, discuss the pros and cons of each method. Are the different classifiers more appropriate for answering different kinds of questions about the election?
```{r warning = FALSE, fig.height = 3, fig.width = 5}
#par(mfrow = c(1,1))
tree.test <- predict(prunedtree, test, type="vector")
#prediction variables
#pred.tree <- prediction(tree.test, train1)
#pred.log <- prediction(prob.test, tst.clY)

#performance indicators
#perf.tree = performance()
#perf.log = performance(y_hat_glm, measure="tpr", x.measure="fpr")

#plotting ROC Curves
# plot(tree.test, col="red", lwd=2, main="ROC curve") 
# plot(tree.test, col="blue",  lwd=2,  add=TRUE) 
# legend(.6,.4, legend=c("Tree", "Logistic"),
#        col=c("red", "green"))
# abline(0,1)

test_prob = predict(fit_glm, newdata = test, type = "response")
test_roc = roc(test$candidate ~ test_prob, plot = TRUE, print.auc = TRUE)
#test_roc2 = roc(test$candidate ~ tree.test[1:921,], plot = TRUE, print.auc = TRUE)
cat('The logistic model has an AUC score of', as.numeric(test_roc$auc),'. A good model will have a high AUC, that is as often as possible a high sensitivity and specificity.')
```

$\rightarrow$ **After fitting the census data onto both the decision tree and logistic regression model, I would conclude that logistic regression seems to be a better model than the pruned tree as our logistic regression curve has a larger area. However, we must remember that the benefits of the decision tree model lies in its inherent simplicity and intrepretability. When it comes to election and polling data, who is the audience? How important is accuracy and is ~5-10% accuracy a fair price to pay for intrepretibility? These are all unique questions relevant to the statistician or company itself and why no model is perfect! To find the "best -fitting" model, one needs to consider a tremendous number of factors: both within the dataset and in the context of the project.  **

# Taking it further

21. This is an open question. Interpret and discuss any overall insights gained in this analysis and possible explanations. Use any tools at your disposal to make your case: visualize errors on the map, discuss what does or doesn't seem reasonable based on your understanding of these methods, propose possible directions (for example, collecting additional data or domain knowledge).  In addition, propose and tackle _at least_ one more interesting question. Creative and thoughtful analyses will be rewarded! 

<!-- Some possibilities for further exploration are: -->

<!--   * Data preprocessing: we aggregated sub-county level data before performing classification. Would classification at the sub-county level before determining the winner perform better? What implicit assumptions are we making? -->

<!--   * Exploring one or more additional classification methods: KNN, LDA, QDA, random forest, boosting, neural networks. (You may research and use methods beyond those covered in this course). How do these compare to logistic regression and the tree method? -->

<!--   * Use linear regression models to predict the `total` vote for each candidate by county.  Compare and contrast these results with the classification models.  Which do you prefer and why?  How might they complement one another? -->

<!--   * Conduct an exploratory analysis of the "purple" counties-- the counties which the models predict Clinton and Trump were roughly equally likely to win.  What is it about these counties that make them hard to predict? -->

<!--   * Instead of using the native attributes (the original features), we can use principal components to create new (and lower dimensional) sets of features with which to train a classification model. This sometimes improves classification performance.  Compare classifiers trained on the original features with those trained on PCA features.   -->

$\rightarrow$ **Throughout this project, we have explored and applied numerous supervised and unsupervised methods on 2016 election & census data. Finishing this project showed me that the process behind electoral predictions is fairly complicated- I'm sure that industry deployed models have a tremendous amount of predictors. Furthermore, our analysis confirms common ideas surrounding political affiliation - predictors relevant to poverty levels, income, and percentage minority contribute a large amount to the county and sub-county predictions. Conversely, unexpected predictors such as are shown to have great impact in our logistic regression model.**

**Although this data was already very "clean" and relatively easy to work with, there were still small struggles throughout the project. Specifically, I found that the imported census and election datasets have misnamed column names, or . Thankfully, these issues were easily solved via utilizing the "tolower", "toupper" and "colnames" functions. **

**Analyzing past statistics is great, but how can we continue this project and what are its implications for future presidential polling issues? I would argue that we could incorporate more information out of this model surrounding the political affiliations between men and women, minorities & whites, e.t.c. Furthermore, one could explore connections between election results and unique variables such as commuting times and workplace location factors.**
  
$\bullet$ **In terms of taking it further, I will be running applying a KNN model with 8-fold cross validation on the election_county the and comparing the errors of the tree, logistic regression, and KNN methods to have a better idea of the pros and cons between different learning models.**

```{r}
# features
x_mx <- train %>%
  select(-candidate) %>%
  as.matrix()

# response
y <- train %>% pull(candidate)

# 10-nearest neighbors
y_hat <- knn(train = x_mx, test = x_mx, cl = y, k = 10)

# leave-one-out CV predictions
y_hat_cv <- knn.cv(train = x_mx, cl = y, k = 10)

# re-train
y_hat_knn <- y_hat
```

$\bullet$ **Above, we trained out election_county dataframe with the KNN method, utilizing cross-fold validation to find the optimal k - value, and then retraining our KNN model with that best k-value. Our KNN error table is shown below:**

```{r}
# compute predictions
preds_knn <- knn(train = x_mx, 
             test = x_mx, 
             cl = y, k = 10)

pred_df <-  bind_cols(pred_knn = as.numeric(preds_knn)) %>% group_by(pred_knn) %>% unique() %>% show
```


```{r}
# cross-tabulate
errors_knn <- table(y, y_hat_knn) %>% show()
```

$\rightarrow$ **With an accuracy rate of and false positive rate of on the test data, the KNN method seems to be lack performance when compared to the logistic regression model with its lower accuracy rate of 0.89 and higher false positive rate of 0.24. It seems that our intuition and our choice of models above was correct!**


